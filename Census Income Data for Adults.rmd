RAGHVENDRA SINGH SHAKTAWAT


```{r}
library(tidyverse)
library(tidyr)
library(stringr)
library(klaR)
library(psych)
library(forcats)
library(caret)
library(corrplot)
```


CENUS INCOME DATA FOR ADULTS


I downloaded the data set Census Income Data for Adults along with its explanation. There are two data sets (adult.data and adult.test).

```{r}
setwd("C:/Users/raghv/Downloads")
adultd_df <- read.csv("adult.data.csv")
head(adultd_df)
dim(adultd_df)
```

I downloaded the two dataset files and loaded two data frames into the R Studio

```{r}
setwd("C:/Users/raghv/Downloads")
adult_df <- read.csv("adult.test.csv", header = FALSE)
adultt_df <- adult_df[-1,]
head(adultt_df)
dim(adultt_df)
```

```{r}
colnames(adultd_df) <- c("Age", "Working_class", "fnlwgt", "Education", "Education_number", "Marital_status","Occupation", "Relation", "Race", "Sex", "Capital_gain", "Capital_loss", "Hours.Per.Week", "Native_country", "Income")
head(adultd_df)
```

I assigned names to the different columns present in the data frames so that the names of these columns can be used below.

```{r}
colnames(adultt_df) <- c("Age", "Working_class", "fnlwgt", "Education", "Education_number", "Marital_status","Occupation", "Relation", "Race", "Sex", "Capital_gain", "Capital_loss", "Hours.Per.Week", "Native_country", "Income")
head(adultt_df)
```

Finally I joined the two data frames to produce a single data frame named adult.

```{r}
adult <- rbind(adultd_df,adultt_df)
head(adult)
dim(adult)
```


I explored the structure of the data set where it is evident that the columns are either characters or integers. 

```{r}
str(adult)
```

```{r}
glimpse(adult)
```

There are 48841 rows and 15 columns and the summary of the data set is also presented below. 

```{r}
summary(adult)
```

I felt it necessary to factorize the columns because I was getting an error of not converting the columns into factors and thus I factorized various columns which can be seen as follows.

```{r}
factor_colummns <- c("Age", "Working_class", "Education", "Race", "Sex", "Native_country", "Income")
```

```{r}
adult[,factor_colummns] <- lapply(adult[,factor_colummns], factor)
```

I made some changes to the data set as it is necessary to convert some columns into numeric and character forms as well as I converted the Income column into factor.

```{r}
adult$Age <- as.numeric(adult$Age)
```

```{r}
adult$Income <- as.character(adult$Income)
```

```{r}
adult$Income <- str_trim(adult$Income)
```

```{r}
adult$Income <- as.factor(adult$Income)
```

In the income column, the distribution is of four types and thus I combined two columns of similar type into one so as to make the data structure more organized

```{r}
adult$Income <- fct_collapse(adult$Income, "<=50K" = c("<=50K", "<=50K."))
```

```{r}
#Collapsing into two classes
adult$Income <- fct_collapse(adult$Income, ">50K" = c(">50K", ">50K."))
```

Then, I used bins that were varying in size from 0 to 70 with an equal intervals of 10 and created a separate bin column for age named Age.bins.

```{r}
bin_size <- c(0,10,20,30,40,50,60,70)
```

```{r}
adult$Age.bins <- cut(adult$Age, breaks = bin_size, right = T)
head(adult)
```


Here I split the data into training and validation set with a ratio of 70 and 30 for the training and validation dataset, respectively. I also used the set.seed size of 100 and here, the training data set contains 34189 rows and the validation data set consists of 14652 rows and 16 columns



```{r}
set.seed(100)
train.size <- 0.7
train.index <- sample.int(nrow(adult), round(nrow(adult) * train.size))
records.train <- adult[train.index,]
records.validation <- adult[-train.index,]
head(records.train)
head(records.validation)
dim(records.train)
dim(records.validation)
```




Here, I created a binary classifier as I converted the income group into factor and assigned the values of zero and one depending on whether it is more or less than “<=50k”

```{r}
records.train$Income <- as.factor(ifelse(records.train$Income == "<=50K", 0, 1))
```

```{r}
records.validation$Income <- as.factor(ifelse(records.validation$Income == "<=50K", 0, 1))
```

Then, I used the NaiveBayes classification algorithm and predicted the values which are mentioned below. I also ignored the other features of the model and transformed continuous variables into categorical variables as I already performed the process of billing. Also, equal sized bins from minimum to maximum which ranged from 0 to 70 with an equal intervals of 10 were used.

```{r}
classifier <- NaiveBayes(Income ~ Age.bins + Education + Working_class + Sex + Race + Native_country, data = records.train)
```

```{r}
predicted <- predict(classifier, records.validation)
```

Here, I built a confusion matrix for the classifier and on building the confusion matrix, it can be seen that it has an accuracy value of .80 and the P value of < 2.2e-16. Other features such as sensitivity, specificity, prevalence, balanced accuracy, etc. are also mentioned above. 

```{r}
confusion <- confusionMatrix(predicted$class, records.validation$Income)
confusion
```

I also calculated the overall values which determines the accuracy and different values of accuracy which can be seen here. The balanced accuracy of this model is .67 and the overall accuracy is 8.012558e-01.

```{r}
conf <- confusion$overall
conf
```

```{r}
log_reg_mod <- glm(data = records.train, Income ~ Age.bins + Education + Working_class + Sex + Race + Native_country, family = "binomial")
```

The coefficients can be seen for the model here:

```{r}
log_reg_mod
```

I also used the response as the type here to calculate only the positive values because without using the response type, the negative values were also getting calculated here in the prediction of the logistic regression model.

```{r}
log_reg_mod_prediction <- predict(log_reg_mod, records.validation, type = "response")
log_reg_mod_prediction
```


Now, I factorized the logistic regression model and the values that I predicted for it into two levels of zero and one.

```{r}
log_reg_mod_predictions_out <- factor(c(ifelse(log_reg_mod_prediction >= 0.5,
1, 0)), levels = c(0, 1))
log_reg_mod_predictions_out
``` 

```{r}
log_reg_mod_confusion <- confusionMatrix(log_reg_mod_predictions_out, records.validation$Income)
```

```{r}
log_reg_mod_confusion
```

Here, I applied the confusion matrix where the accuracy is .806 and the P value is < 2.2e-16. The overall features of this model are as follows:

```{r}
log_reg <- log_reg_mod_confusion$overall
log_reg
```


Now, I am building a function called predictEarningsClass() that predicts whether an individual makes more or less than US$50,000 and that combines the two predictive models into a simple ensemble. 

```{r}
predictEarningsClass <- function(predicted, log_reg_mod_prediction, conf = conf, log_reg = log_reg){
  return(factor(ifelse((predicted == 1) & (log_reg_mod_prediction == 1), 1,
                ifelse((predicted == 0) & (log_reg_mod_prediction == 0), 0,
                ifelse(predicted == 1 & log_reg_mod_prediction == 0 & (conf > log_reg), 1,
                ifelse(predicted == 0 & log_reg_mod_prediction == 1 & (conf < log_reg), 1, 0))))))
}
```

Here, I built a function called predictEarningsClass. Here, I used the models that I created above which were the predicted logistic regression model and its predictions and also used the if else command with two values of 0 and 1 to make the function produce the desired output which can be utilized below.


Now, I will predict whether a 47-year-old black female adult who is a local government worker with a a Bachelor's degree who immigrated from Honduras earns more or less than US$50,000.

```{r}
female_adult <- data.frame(Age = 47, Working_class = " Local-gov", Education = " Bachelors",  Race = " Black", Sex = " Female", Native_country = " Honduras", stringsAsFactors = TRUE)
female_adult
```

Also, I created a new data frame named female_adult and used the same feature of binning that I used above with the range from 0 to 70 and equally spaced intervals of 10.
```{r}
bins <- c(0,10,20,30,40,50,60,70)
```

```{r}
female_adult$Age.bins<- cut(female_adult$Age, breaks = bins, right = T)
female_adult
```

```{r}
female_adult_prediction <- predict(classifier, female_adult)
female_adult_prediction
```

Then, I trained the model by using the predict function which produced two values for both the levels of zero and one at 0.9940669 and 0.00593311, respectively. 

```{r}
female_adult_prediction_out <- predict(log_reg_mod, female_adult, type = "response")
```

```{r}
female_adult_prediction_out
```


I also used the if else function with two levels of zero and one so as to determine the output.

```{r}
female_adult_prediction_out_l <- factor(c(ifelse(female_adult_prediction_out  >= 0.5, 1, 0)), levels = c(0, 1))
```

```{r}
female_adult_prediction_out_l
```

```{r}
final <- predictEarningsClass(unlist(female_adult_prediction), female_adult_prediction_out_l, conf = conf, log_reg = log_reg)
final
```

Here, it is evident that the female adult earns less than US $50,000



THANK YOU

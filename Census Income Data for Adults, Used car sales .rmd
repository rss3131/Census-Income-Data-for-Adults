RAGHVENDRA SINGH SHAKTAWAT


```{r}
library(tidyverse)
library(tidyr)
library(stringr)
library(klaR)
library(psych)
library(forcats)
library(caret)
library(corrplot)
```


CENUS INCOME DATA FOR ADULTS


I downloaded the data set Census Income Data for Adults along with its explanation. There are two data sets (adult.data and adult.test).

```{r}
setwd("C:/Users/raghv/Downloads")
adultd_df <- read.csv("adult.data.csv")
head(adultd_df)
dim(adultd_df)
```

I downloaded the two dataset files and loaded two data frames into the R Studio

```{r}
setwd("C:/Users/raghv/Downloads")
adult_df <- read.csv("adult.test.csv", header = FALSE)
adultt_df <- adult_df[-1,]
head(adultt_df)
dim(adultt_df)
```

```{r}
colnames(adultd_df) <- c("Age", "Working_class", "fnlwgt", "Education", "Education_number", "Marital_status","Occupation", "Relation", "Race", "Sex", "Capital_gain", "Capital_loss", "Hours.Per.Week", "Native_country", "Income")
head(adultd_df)
```

I assigned names to the different columns present in the data frames so that the names of these columns can be used below.

```{r}
colnames(adultt_df) <- c("Age", "Working_class", "fnlwgt", "Education", "Education_number", "Marital_status","Occupation", "Relation", "Race", "Sex", "Capital_gain", "Capital_loss", "Hours.Per.Week", "Native_country", "Income")
head(adultt_df)
```

Finally I joined the two data frames to produce a single data frame named adult.

```{r}
adult <- rbind(adultd_df,adultt_df)
head(adult)
dim(adult)
```


I explored the structure of the data set where it is evident that the columns are either characters or integers. 

```{r}
str(adult)
```

```{r}
glimpse(adult)
```

There are 48841 rows and 15 columns and the summary of the data set is also presented below. 

```{r}
summary(adult)
```

I felt it necessary to factorize the columns because I was getting an error of not converting the columns into factors and thus I factorized various columns which can be seen as follows.

```{r}
factor_colummns <- c("Age", "Working_class", "Education", "Race", "Sex", "Native_country", "Income")
```

```{r}
adult[,factor_colummns] <- lapply(adult[,factor_colummns], factor)
```

I made some changes to the data set as it is necessary to convert some columns into numeric and character forms as well as I converted the Income column into factor.

```{r}
adult$Age <- as.numeric(adult$Age)
```

```{r}
adult$Income <- as.character(adult$Income)
```

```{r}
adult$Income <- str_trim(adult$Income)
```

```{r}
adult$Income <- as.factor(adult$Income)
```

In the income column, the distribution is of four types and thus I combined two columns of similar type into one so as to make the data structure more organized

```{r}
adult$Income <- fct_collapse(adult$Income, "<=50K" = c("<=50K", "<=50K."))
```

```{r}
#Collapsing into two classes
adult$Income <- fct_collapse(adult$Income, ">50K" = c(">50K", ">50K."))
```

Then, I used bins that were varying in size from 0 to 70 with an equal intervals of 10 and created a separate bin column for age named Age.bins.

```{r}
bin_size <- c(0,10,20,30,40,50,60,70)
```

```{r}
adult$Age.bins <- cut(adult$Age, breaks = bin_size, right = T)
head(adult)
```


Here I split the data into training and validation set with a ratio of 70 and 30 for the training and validation dataset, respectively. I also used the set.seed size of 100 and here, the training data set contains 34189 rows and the validation data set consists of 14652 rows and 16 columns



```{r}
set.seed(100)
train.size <- 0.7
train.index <- sample.int(nrow(adult), round(nrow(adult) * train.size))
records.train <- adult[train.index,]
records.validation <- adult[-train.index,]
head(records.train)
head(records.validation)
dim(records.train)
dim(records.validation)
```




Here, I created a binary classifier as I converted the income group into factor and assigned the values of zero and one depending on whether it is more or less than “<=50k”

```{r}
records.train$Income <- as.factor(ifelse(records.train$Income == "<=50K", 0, 1))
```

```{r}
records.validation$Income <- as.factor(ifelse(records.validation$Income == "<=50K", 0, 1))
```

Then, I used the NaiveBayes classification algorithm and predicted the values which are mentioned below. I also ignored the other features of the model and transformed continuous variables into categorical variables as I already performed the process of billing. Also, equal sized bins from minimum to maximum which ranged from 0 to 70 with an equal intervals of 10 were used.

```{r}
classifier <- NaiveBayes(Income ~ Age.bins + Education + Working_class + Sex + Race + Native_country, data = records.train)
```

```{r}
predicted <- predict(classifier, records.validation)
```

Here, I built a confusion matrix for the classifier and on building the confusion matrix, it can be seen that it has an accuracy value of .80 and the P value of < 2.2e-16. Other features such as sensitivity, specificity, prevalence, balanced accuracy, etc. are also mentioned above. 

```{r}
confusion <- confusionMatrix(predicted$class, records.validation$Income)
confusion
```

I also calculated the overall values which determines the accuracy and different values of accuracy which can be seen here. The balanced accuracy of this model is .67 and the overall accuracy is 8.012558e-01.

```{r}
conf <- confusion$overall
conf
```

```{r}
log_reg_mod <- glm(data = records.train, Income ~ Age.bins + Education + Working_class + Sex + Race + Native_country, family = "binomial")
```

The coefficients can be seen for the model here:

```{r}
log_reg_mod
```

I also used the response as the type here to calculate only the positive values because without using the response type, the negative values were also getting calculated here in the prediction of the logistic regression model.

```{r}
log_reg_mod_prediction <- predict(log_reg_mod, records.validation, type = "response")
log_reg_mod_prediction
```


Now, I factorized the logistic regression model and the values that I predicted for it into two levels of zero and one.

```{r}
log_reg_mod_predictions_out <- factor(c(ifelse(log_reg_mod_prediction >= 0.5,
1, 0)), levels = c(0, 1))
log_reg_mod_predictions_out
``` 

```{r}
log_reg_mod_confusion <- confusionMatrix(log_reg_mod_predictions_out, records.validation$Income)
```

```{r}
log_reg_mod_confusion
```

Here, I applied the confusion matrix where the accuracy is .806 and the P value is < 2.2e-16. The overall features of this model are as follows:

```{r}
log_reg <- log_reg_mod_confusion$overall
log_reg
```


Now, I am building a function called predictEarningsClass() that predicts whether an individual makes more or less than US$50,000 and that combines the two predictive models into a simple ensemble. 

```{r}
predictEarningsClass <- function(predicted, log_reg_mod_prediction, conf = conf, log_reg = log_reg){
  return(factor(ifelse((predicted == 1) & (log_reg_mod_prediction == 1), 1,
                ifelse((predicted == 0) & (log_reg_mod_prediction == 0), 0,
                ifelse(predicted == 1 & log_reg_mod_prediction == 0 & (conf > log_reg), 1,
                ifelse(predicted == 0 & log_reg_mod_prediction == 1 & (conf < log_reg), 1, 0))))))
}
```

Here, I built a function called predictEarningsClass. Here, I used the models that I created above which were the predicted logistic regression model and its predictions and also used the if else command with two values of 0 and 1 to make the function produce the desired output which can be utilized below.


Now, I will predict whether a 47-year-old black female adult who is a local government worker with a a Bachelor's degree who immigrated from Honduras earns more or less than US$50,000.

```{r}
female_adult <- data.frame(Age = 47, Working_class = " Local-gov", Education = " Bachelors",  Race = " Black", Sex = " Female", Native_country = " Honduras", stringsAsFactors = TRUE)
female_adult
```

Also, I created a new data frame named female_adult and used the same feature of binning that I used above with the range from 0 to 70 and equally spaced intervals of 10.
```{r}
bins <- c(0,10,20,30,40,50,60,70)
```

```{r}
female_adult$Age.bins<- cut(female_adult$Age, breaks = bins, right = T)
female_adult
```

```{r}
female_adult_prediction <- predict(classifier, female_adult)
female_adult_prediction
```

Then, I trained the model by using the predict function which produced two values for both the levels of zero and one at 0.9940669 and 0.00593311, respectively. 

```{r}
female_adult_prediction_out <- predict(log_reg_mod, female_adult, type = "response")
```

```{r}
female_adult_prediction_out
```


I also used the if else function with two levels of zero and one so as to determine the output.

```{r}
female_adult_prediction_out_l <- factor(c(ifelse(female_adult_prediction_out  >= 0.5, 1, 0)), levels = c(0, 1))
```

```{r}
female_adult_prediction_out_l
```

```{r}
final <- predictEarningsClass(unlist(female_adult_prediction), female_adult_prediction_out_l, conf = conf, log_reg = log_reg)
final
```

Here, it is evident that the female adult earns less than US $50,000



USED CAR SALES DATASET



```{r}
#Loading the dataset
setwd("C:/Users/raghv/Downloads")
cars.df <- read.csv("used_car_sales_ebay.csv")
head(cars.df)
dim(cars.df)
```

After unloading the data set and creating its data frame into my R studio, I explored the data.

```{r}
str(cars.df)
```

The columns are mainly in the integer and character form and the summary of the data set is present below.

```{r}
glimpse(cars.df)
```

```{r}
summary(cars.df)
```

Here, I removed the columns that are mentioned and created a new data frame which has six columns i.e., pricesold, yearsold, Mileage, Year, NumCylinders and DriveType.

```{r}
cars_df <- subset(cars.df, select = -c(1, 4, 6, 7, 9, 10, 11))
head(cars_df)
```

```{r}
colSums(is.na(cars_df))
```

This is to check the na values and I also eliminated the row that have zero value in NumCylinders column and also checked for the NA values in the drive type column.

```{r}
cars_df <- cars_df[cars_df$NumCylinders != 0, ]
```

```{r}
table(is.na(cars_df$DriveType))
```

```{r}
cars_df <- cars_df %>% drop_na()
head(cars_df)
```

I also dropped all the NA values to make the data set much more cleaner and converted the drive type column into a binary factor column with two values of zero and one.

```{r}
cars_df$DriveType <- factor(ifelse(cars_df$DriveType == "RWD", 1, 0))
head(cars_df)
```


Taking care of outliers.


```{r}
min(cars_df$pricesold)
max(cars_df$pricesold)
```

```{r}
min(cars_df$Mileage)
max(cars_df$Mileage)
```

```{r}
min(cars_df$NumCylinders)
max(cars_df$NumCylinders)
```

First of all, I checked for the minimum and maximum values for different columns such as pricesold, Mileage and NumCylinders.

```{r}
summary(cars_df$Mileage)
```

I also visualized the outliers for the pricesold and Mileage column which can be seen in the form of a box plot here.

```{r}
ggplot(cars_df, aes(pricesold)) + geom_boxplot(outlier.colour = "red", color = "blue") + labs(x = "pricesold")
```

```{r}
ggplot(cars_df, aes(Mileage)) + geom_boxplot(outlier.colour = "green", color = "blue") + labs(x = "Mileage")
```

Also I am taking a major value of those cars that traveled more than 300000 miles as outliers and choose to remove them from the data set.

```{r}
cars.df <- cars_df
cars.no.df <- cars.df %>% filter(Mileage > 300000)
```

```{r}
mean_po <- mean(cars.no.df$pricesold)
mean_po
```

```{r}
sd_po <- sd(cars.no.df$pricesold)
sd_po
```

```{r}
diff <- (mean_po - (3 * sd_po))
diff
```

```{r}
add <- (mean_po + (3 * sd_po))
add
```

Finally, I calculated the mean and standard deviation for the price sold column and performed the Z score elimination procedure to further make the data sets free of outliers.

```{r}
cars.no.df <- cars.no.df %>% filter((pricesold > diff) & (pricesold < add))
head(cars.no.df)
```

```{r}
cars.no.df <- cars.no.df %>% filter((Mileage <= 500000) & (Year <= 2020) & (Year >= 1900) & (NumCylinders <= 16))
nrow(cars.no.df)
```

Now I am using pairs.panel, to show the distributions of each of the numeric features in the data set with outliers removed 

```{r}
pairs.panels(cars.no.df)
```

I installed the KLAR package and used pairs.panel to visualize the distribution of the numeric features that are present in the data set after removing the outliers.

```{r}
ks.test(cars.no.df$pricesold, y = "pnorm")
ks.test(cars.no.df$yearsold, y = "pnorm")
ks.test(cars.no.df$Mileage, y = "pnorm")
ks.test(cars.no.df$Year, y = "pnorm")
ks.test(cars.no.df$NumCylinders, y = "pnorm")
```

For checking the normality of each column, I used two different methods one being the Kolmogorov-Smirnov test and another is the Shapiro-Wilk normality test.

```{r}
shapiro.test(cars.no.df$pricesold)
shapiro.test(cars.no.df$yearsold)
shapiro.test(cars.no.df$Mileage)
shapiro.test(cars.no.df$Year)
shapiro.test(cars.no.df$NumCylinders)
```

```{r}
log_pricesold <- log1p(cars.no.df$pricesold)
log_Mileage <- log1p(cars.no.df$Mileage)
log_Year <- log1p(cars.no.df$Year)
log_NumCylinders <- log1p(cars.no.df$NumCylinders)
```

Also,I’d normalize the features using the log as I converted some of the columns into log to normalize them and thus created a new data frame which possess log values for those columns.

```{r}
data_frame_new <- data.frame(log_pricesold, log_Mileage, log_Year, log_NumCylinders)
head(data_frame_new)
```

```{r}
data_frame_merge <- cbind(cars.no.df, data_frame_new)
head(data_frame_merge)
```
Finally I merged the original data frame with the newly created log data frame and named it as data_frame_merge.

```{r}
cars.tx <- subset(data_frame_merge, select = c(2, 6, 7, 8, 9, 10))
head(cars.tx)
```

```{r}
cars.no.df$DriveType <- as.numeric(cars.no.df$DriveType)
```


```{r}
correlations <- cor(cars.no.df[c("pricesold", "yearsold", "Mileage", "Year", "NumCylinders", "DriveType")], method = "pearson",use = "complete.obs")
correlations
```

To check the correlations, I use the cor function and also plotted the correlations which can be seen in the visualization below.

```{r}
corrplot(correlations, method="color")
```

The maximum correlation of mileage can be seen with the price sold column as compared to the other columns. However the correlation is still not very strong but it is comparatively stronger than the other columns.



Now,I am splitting each of the three data sets, cars.no.df, cars.df, and cars.tx 75%/25% to retain 25% for testing using random sampling without replacement. I will also call the data sets, cars.training and cars.testing, cars.no.training and cars.no.testing, and cars.tx.training and cars.tx.testing.

```{r}
set.seed(100)
train.size <- 0.75
train.index <- sample.int(nrow(cars.no.df), round(nrow(cars.no.df) * train.size))
cars.no.training <- cars.no.df[train.index,]
cars.no.testing <- cars.no.df[-train.index,]
head(cars.no.training)
head(cars.no.testing)
dim(cars.no.training)
dim(cars.no.testing)
```


```{r}
set.seed(100)
train.size <- 0.75
train.index <- sample.int(nrow(cars.df), round(nrow(cars.df) * train.size))
cars.training <- cars.df[train.index,]
cars.testing <- cars.df[-train.index,]
head(cars.training)
head(cars.testing)
dim(cars.training)
dim(cars.testing)
```


```{r}
set.seed(100)
train.size <- 0.75
train.index <- sample.int(nrow(cars.tx), round(nrow(cars.tx) * train.size))
cars.tx.training <- cars.tx[train.index,]
cars.tx.testing <- cars.tx[-train.index,]
head(cars.tx.training)
head(cars.tx.testing)
dim(cars.tx.training)
dim(cars.tx.testing)
```

Here, I split the three data sets into a ratio of 75 to 25% for the training and testing data sets, respectively.
I also found it necessary to convert the DriveType column into factor form with the factors of zero and one value because I was getting error so I converted this column into factor form.

```{r}
cars.no.df$DriveType <- factor(ifelse(cars.no.df$DriveType == "2", 1, 0))
```


Now, I am building three full multiple regression models for predicting Mileage: one with cars.training, one with cars.no.training, and one with cars.tx.training, i.e., regression models that contains all features regardless of their p-values. I will call the models reg.full, reg.no, and reg.tx.

```{r}
reg.full <- lm(Mileage ~ ., data = cars.training)
reg.no <- lm(Mileage ~ ., data = cars.no.training)
reg.tx <- lm(log_Mileage ~ ., data = cars.tx.training)
```

Here, I built three full multiple regression models for predicting the mileage and the summary is presented below with different values of P and residual Standard error.

```{r}
summary(reg.full)
summary(reg.no)
summary(reg.tx)
```


I am building three ideal multiple regression models for cars.training, cars.no.training, and cars.tx.training using backward elimination based on p-value for predicting Mileage.

```{r}
reg.full.backward.a <- lm(data = cars.training, Mileage ~ .)
reg.full.backward.a
```

```{r}
summary(reg.full.backward.a)
```

Above one is the first linear model. Here, I used the entire data set and the lm function and presented a summary of it. The lm function is used to formula mode where an independent variable is used in relation to the predictor variables. Here, Mileage is used in relation to the other columns of the data set. The summary of this model shows the collection of estimates. The relationship between the coefficients is displayed here. The negative correlation indicates an inversely proportional relationship between the two variables and columns as if the value of one goes up, the other goes down. Whereas, in a positive correlation, the values are in a directly proportional relationship, which means that the values of both the columns will increase or decrease simultaneously. If the value is far away from 1, it indicates a weak correlation. The * symbol indicates the degree of confidence in the value i.e., the probability of whether or not we can trust this value and the probability of rejecting this value is quite low. Also, there are some coefficients which have the highest probability of getting rejected. I am going to reject them in the coming models to create a final model.

```{r}
reg.full.backward.b <- lm(data = cars.training, Mileage ~ yearsold + Year + NumCylinders + DriveType)
reg.full.backward.b
```

```{r}
summary(reg.full.backward.b)
```

Here, I used backward elimination procedure to eliminate the the worst or least trusted variable, one at a time. In the above model, that eliminated variable is “pricesold” with a p-value of 0.99082 . The reason for eliminating it is because of its highest p value among the other values.

```{r}
reg.full.backward.c <- lm(data = cars.training, Mileage ~ yearsold + Year + DriveType)
reg.full.backward.c
```

```{r}
summary(reg.full.backward.c)
```

After eliminating pricesold variable in the above model, the R-squared is almost identical as it improved a bit but is almost insignificant. But the variable NumCylinders is eliminated which does not contribute much to the final formula. The next variable that I am going to eliminate is “Year” with a value of 0.92831  ( more than the other values present ) which is eliminated in the below model.

```{r}
reg.full.backward.d <- lm(data = cars.training, Mileage ~ yearsold + DriveType)
reg.full.backward.d
```

```{r}
summary(reg.full.backward.d)
```

Now, I will continue the backward elimination process and remove other features from the model below:

```{r}
reg.no.backward.a <- lm(data = cars.no.training, Mileage ~ .)
reg.no.backward.a
```

```{r}
summary(reg.no.backward.a)
```

```{r}
reg.no.backward.b <- lm(data = cars.no.training, Mileage ~ pricesold + yearsold + Year + NumCylinders)
reg.no.backward.b
```

```{r}
summary(reg.no.backward.b)
```

```{r}
reg.no.backward.c <- lm(data = cars.no.training, Mileage ~ pricesold + yearsold + Year)
reg.no.backward.c
```

```{r}
summary(reg.no.backward.c)
```


```{r}
reg.no.backward.d <- lm(data = cars.no.training, Mileage ~ pricesold + Year)
reg.no.backward.d
```

```{r}
summary(reg.no.backward.d)
```

```{r}
reg.tx.backward.a <- lm(data = cars.tx.training, log_Mileage ~ .)
reg.tx.backward.a
```

```{r}
summary(reg.tx.backward.a)
```

```{r}
reg.tx.backward.b <- lm(data = cars.tx.training, log_Mileage ~ yearsold + DriveType + log_pricesold + log_Year)
reg.tx.backward.b
```

```{r}
summary(reg.tx.backward.b)
```

```{r}
reg.tx.backward.c <- lm(data = cars.tx.training, log_Mileage ~ yearsold + log_pricesold + log_Year)
reg.tx.backward.c
```

```{r}
summary(reg.tx.backward.c)
```

```{r}
reg.tx.backward.d <- lm(data = cars.tx.training, log_Mileage ~ log_pricesold + log_Year)
reg.tx.backward.d
```

```{r}
summary(reg.tx.backward.d)
```



In order to provide the analysis of six models using their testing data sets, R-squared value and RMSE, I used the predict function as well as displayed the summary and RMSE value of it. 

Model 1

```{r}
m1 <- predict(reg.full, cars.testing)
summary(reg.full)$r.squared
```

```{r}
RMSE(cars.testing$Mileage, m1)
```

Model 2

```{r}
m2 <- predict(reg.full.backward.d, cars.testing)
summary(reg.full.backward.d)$r.squared
```

```{r}
RMSE(cars.testing$Mileage, m2)
```

Model 3

```{r}
m3 <- predict(reg.no, cars.no.testing)
summary(reg.no)$r.squared
```

```{r}
RMSE(cars.no.testing$Mileage, m3)
```

Among the first three models the maximum RMS value is of model 1 with a value of 28638423.


Model 4

```{r}
m4 <- predict(reg.no.backward.d, cars.no.testing)
summary(reg.no.backward.d)$r.squared
```

```{r}
RMSE(cars.no.testing$Mileage, m4)
```

Model 5

```{r}
m5 <- predict(reg.tx, cars.tx.testing)
summary(reg.tx)$r.squared
```

```{r}
RMSE(cars.tx.testing$log_Mileage, m5)
```

Model 6

```{r}
m6 <- predict(reg.tx.backward.d, cars.tx.testing)
summary(reg.tx.backward.d)$r.squared
```

```{r}
RMSE(cars.tx.testing$log_Mileage, m6)
```

The least RMSE value among all the models is of Model 3 at a value of 44494.95.


```{r}
chevrolet_blazer <- data.frame(Year = 1999, NumCylinders = 6, DriveType  = 0, yearsold = 2019, pricesold = 9450)
chevrolet_blazer
```
Here, I first created a new data frame and added all the columns as per the values that are assigned here.

```{r}
chevrolet_blazer$DriveType <- as.factor(chevrolet_blazer$DriveType)
```

I had to convert the DriveType column into factor so as to proceed here because all the columns were present in numeric form and it was important to convert it into factor to predict the value as well as to implement value from the previous models.


```{r}
str(chevrolet_blazer)
```

```{r}
predict(reg.full, chevrolet_blazer)
predict(reg.full.backward.d, chevrolet_blazer)
```

```{r}
chevrolet_blazer$DriveType <- as.numeric(chevrolet_blazer$DriveType)
predict(reg.no, chevrolet_blazer)
predict(reg.no.backward.d, chevrolet_blazer)
```

```{r}
log_chevrolet_blazer <- data.frame(log_Year = log1p(1999), log_NumCylinders = log1p(6), DriveType  = 0, yearsold = 2019, log_pricesold = log1p(9450))
log_chevrolet_blazer
```

I also found it necessary to use the log values to normalize the features and used the predicted odometer readings/ Mileage of Chevrolet blazer which can be seen below:

```{r}
log_chevrolet_blazer$DriveType <- as.factor(log_chevrolet_blazer$DriveType)
```

```{r}
predict(reg.tx, log_chevrolet_blazer)
predict(reg.tx.backward.d, log_chevrolet_blazer)
```



THANK YOU